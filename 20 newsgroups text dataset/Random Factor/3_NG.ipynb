{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Saman Paidar Nia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All resources are listed at the bottom of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get important libraries for this class.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import math\n",
    "import sys\n",
    "import logging\n",
    "#-----------------------------------------------------------\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from sklearn.preprocessing import normalize\n",
    "from numpy import linalg as LA\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from math import sqrt\n",
    "#------------------------------------------------------------\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from optparse import OptionParser\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(X):\n",
    "    return normalize(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(A):\n",
    "    S = np.sum(A, 0)\n",
    "    D = np.diag(S)\n",
    "    D = LA.matrix_power(D, -1)\n",
    "    L = np.dot(D, A)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(V):\n",
    "    return (V - min(V)) / (max(V) - min(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correlation_Similarity:\n",
    "    def get_matrix(self, Data):\n",
    "        X = standardization(Data)\n",
    "        X = pdist(X, 'correlation')\n",
    "        X = squareform(X)\n",
    "        L = laplacian(X)\n",
    "        Y = np.apply_along_axis(normalization, 1, L)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cosine_Similarity:\n",
    "    def get_matrix(self, Data):\n",
    "        X = standardization(Data)\n",
    "        X = pdist(X, 'cosine')\n",
    "        X = squareform(X)\n",
    "        L = laplacian(X)\n",
    "        Y = np.apply_along_axis(normalization, 1, L)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity_Dataset_Iterator():\n",
    "    def __init__(self, data, labels, similarity):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.matrix = similarity.get_matrix(data)\n",
    "        self.data_size = self.matrix.shape[0]\n",
    "        self.current_index = 0\n",
    "    def next_batch(self, num):\n",
    "        data=self.matrix.transpose()\n",
    "        labels=self.labels\n",
    "        idx = np.arange(0 , len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:num]\n",
    "        data_shuffle = [data[ i] for i in idx]\n",
    "        labels_shuffle = [labels[ i] for i in idx]\n",
    "        return data_shuffle, labels_shuffle\n",
    "    def whole_dataset(self):\n",
    "        return (self.matrix.transpose(), self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Using Scikit-Learn libraries to fetching the Newsgroups data set: http://scikit-learn.org\n",
    "def read_NewsGroup_data(similarity):    \n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s %(levelname)s %(message)s')\n",
    "    op = OptionParser()\n",
    "    op.add_option(\"--lsa\", dest=\"n_components\", type=\"int\",\n",
    "                  help=\"Preprocess documents with latent semantic analysis.\")    \n",
    "    op.add_option(\"--no-idf\",action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "                  help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "    op.add_option(\"--use-hashing\", action=\"store_true\", default=False,\n",
    "                  help=\"Use a hashing feature vectorizer\")\n",
    "    op.add_option(\"--n-features\", type=int, default=10000,\n",
    "                  help=\"Maximum number of features to extract from text.\")    \n",
    "    def is_interactive():\n",
    "        return not hasattr(sys.modules['__main__'], '__file__')\n",
    "    argv = [] if is_interactive() else sys.argv[1:]\n",
    "    (opts, args) = op.parse_args(argv)\n",
    "    if len(args) > 0:\n",
    "        op.error(\"this script takes no arguments.\")\n",
    "        sys.exit(1)\n",
    "    categories_3NG = ['comp.graphics', 'rec.sport.baseball', 'talk.politics.guns']\n",
    "    # categories = categories_3NG\n",
    "    dataset = fetch_20newsgroups(subset='train', categories=categories_3NG,\n",
    "                                 shuffle=True, random_state=42)\n",
    "    labels = dataset.target[:600]\n",
    "    true_k = np.unique(labels).shape[0]\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,min_df=2,\n",
    "                                 stop_words='english',use_idf=opts.use_idf)\n",
    "    X = vectorizer.fit_transform(dataset.data[:600])\n",
    "    if opts.n_components:\n",
    "        svd = TruncatedSVD(opts.n_components)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "        X = lsa.fit_transform(X)\n",
    "        explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    return Similarity_Dataset_Iterator(X.toarray(), labels, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Correlation_Similarity as similarity dataset.\n",
    "trainSet_correlation = read_NewsGroup_data(Correlation_Similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Cosine_Similarity as similarity dataset.\n",
    "trainSet_cosine = read_NewsGroup_data(Cosine_Similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = trainSet_correlation.data_size #--------- Number of input data.\n",
    "# Define the number of hidden layer. \n",
    "if n_input >= 1024:\n",
    "    Nn = int(2048)\n",
    "elif n_input >= 512:\n",
    "    Nn = int(1024)\n",
    "elif n_input >= 256:\n",
    "    Nn = int(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = int(Nn/2) #-------------------- The autoencoder hidden layer 1.\n",
    "n_code = str(int(n_hidden_1/2)) #----------- The number of output dimension value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: ----------- 600\n",
      "Layer 2: ----------- 512\n",
      "Layer 3: ----------- 256\n"
     ]
    }
   ],
   "source": [
    "print('Layer 1: -----------', n_input)\n",
    "print('Layer 2: -----------', n_hidden_1)\n",
    "print('Layer 3: -----------', int(n_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_(X, n_clusters):\n",
    "    kmeans_centroids,_ =  kmeans(X, n_clusters)\n",
    "    kmeans_, _ = vq(X, kmeans_centroids)\n",
    "    return kmeans_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, n_code, mode_train):    \n",
    "    with tf.variable_scope(\"encoder\"):        \n",
    "        with tf.variable_scope(\"hidden-layer-1\"):\n",
    "            hidden_1 = layer(x, [n_input, n_hidden_1], [n_hidden_1], mode_train)\n",
    "        with tf.variable_scope(\"embedded\"):\n",
    "            code = layer(hidden_1, [n_hidden_1, n_code], [n_code], mode_train)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(code, n_code, mode_train):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        with tf.variable_scope(\"hidden-layer-1\"):\n",
    "            hidden_1 = layer(code, [n_code, n_hidden_1], [n_hidden_1], mode_train)\n",
    "        with tf.variable_scope(\"reconstructed\"):\n",
    "            output = layer(hidden_1, [n_hidden_1, n_input], [n_input], mode_train)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, mode_train):\n",
    "    beta_initialize = tf.constant_initializer(value=0.1, dtype=tf.float32)\n",
    "    gamma_initialize = tf.constant_initializer(value=0.1, dtype=tf.float32)\n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_initialize)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_initialize)\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    def mean_var():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = control_flow_ops.cond(mode_train, mean_var, lambda: (ema_mean, ema_var))\n",
    "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-08, True)\n",
    "    return tf.reshape(normed, [-1, n_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape, mode_train):\n",
    "    value_initialize = (1.0 / weight_shape[0] ** 0.5)\n",
    "    weight_initialize = tf.random_normal_initializer(stddev = value_initialize, seed = None)\n",
    "    bias_initialize = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    w = tf.get_variable(\"w\", weight_shape, initializer=weight_initialize)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_initialize)\n",
    "    return tf.nn.sigmoid(batch_norm((tf.matmul(input, w) + b), weight_shape[1], mode_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(reconstructed, x):\n",
    "    with tf.variable_scope(\"train\"):\n",
    "        train_loss = tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(reconstructed, x)), 1))\n",
    "        return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, learning_rate, beta1, beta2, global_step):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_layers = 3 #------------------------------ Number of Neural Networks Layers.\n",
    "beta1 = 0.9 #------------------------------- The decay rate 1.  \n",
    "beta2 = 0.999 #----------------------------- The decay rate 2.\n",
    "learning_rate = (beta1/n_input) #----------- The learning rate.\n",
    "n_batch = math.ceil(sqrt(sqrt(n_input))) #-- Number of selection data in per step.\n",
    "n_backpro = math.ceil(n_input/n_batch) #---- Number of Backpro in per epoch.\n",
    "n_clusters = 3 #---------------------------- Number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cor, labels_cor = trainSet_correlation.whole_dataset() #-- Allocation of data and labels\n",
    "data_cos, labels_cos = trainSet_cosine.whole_dataset() #------- Allocation of data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cor=[] #--------------------------- A list to keep all NMI scores.\n",
    "loss_cost_cor=[] #------------------------- A list to keep all training evaluations.\n",
    "seeding_cor=[] #--------------------------- A list to keep all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI score for AE is: 81.14 and new cost is: 70.36 in 1 step of seeding.\n",
      "NMI score for AE is: 85.60 and new cost is: 70.65 in 2 step of seeding.\n",
      "NMI score for AE is: 81.72 and new cost is: 70.64 in 3 step of seeding.\n",
      "NMI score for AE is: 83.27 and new cost is: 70.40 in 4 step of seeding.\n",
      "NMI score for AE is: 83.73 and new cost is: 70.23 in 5 step of seeding.\n",
      "NMI score for AE is: 84.69 and new cost is: 70.49 in 6 step of seeding.\n",
      "NMI score for AE is: 84.04 and new cost is: 70.84 in 7 step of seeding.\n",
      "NMI score for AE is: 83.32 and new cost is: 70.96 in 8 step of seeding.\n",
      "NMI score for AE is: 82.44 and new cost is: 70.16 in 9 step of seeding.\n",
      "NMI score for AE is: 83.71 and new cost is: 70.80 in 10 step of seeding.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    with tf.Graph().as_default():    \n",
    "        with tf.variable_scope(\"autoencoder_architecture\"):\n",
    "            x = tf.placeholder(\"float\", [None, n_input])   \n",
    "            mode_train = tf.placeholder(tf.bool)\n",
    "            code = encoder(x, int(n_code), mode_train)\n",
    "            reconstructed = decoder(code, int(n_code), mode_train)\n",
    "            cost = loss(reconstructed, x)\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
    "            sess = tf.Session()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            # Training cycle\n",
    "            for ii in range(n_layers):\n",
    "                # Fit training with backpropagation using batch data.\n",
    "                for jj in range(n_backpro):\n",
    "                    miniData, _ = trainSet_correlation.next_batch(n_batch)\n",
    "                    _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n",
    "                                                                              mode_train: True})       \n",
    "                #------------------------- End of the Optimization ------------------------------\n",
    "                \n",
    "    # Getting embedded codes and running K-Means on them.\n",
    "    ae_codes_cor = sess.run(code, feed_dict={x: data_cor, mode_train: False})        \n",
    "    idx_cor = k_means_(ae_codes_cor, n_clusters)\n",
    "    ae_nmi_cor = normalized_mutual_info_score(labels_cor, idx_cor)\n",
    "    ae_nmi_cor = ae_nmi_cor*100\n",
    "    results_cor.append(ae_nmi_cor)    \n",
    "    seeding_cor.append(i)\n",
    "    loss_cost_cor.append(new_cost)    \n",
    "    print(\"NMI score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step of seeding.\"\n",
    "          .format(ae_nmi_cor, new_cost, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average of NMI Score for >>> 10 <<< Random Factors in Autoencoder Correlation is >>> 83.37 <<<\n"
     ]
    }
   ],
   "source": [
    "print(\"The Average of NMI Score for >>> {:d} <<< Random Factors in Autoencoder Correlation is >>> {:0.2f} <<<\"\n",
    "      .format(len(seeding_cor), (np.mean(results_cor))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[81.141579366316279,\n",
       " 85.597559650628142,\n",
       " 81.72329841690204,\n",
       " 83.27027416571859,\n",
       " 83.725137221212137,\n",
       " 84.692385276772825,\n",
       " 84.041285866078738,\n",
       " 83.324720546567903,\n",
       " 82.441775144916434,\n",
       " 83.713348790719621]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cos=[] #--------------------------- A list to keep all NMI scores.\n",
    "loss_cost_cos=[] #------------------------- A list to keep all training evaluations.\n",
    "seeding_cos=[] #--------------------------- A list to keep all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI score for AE is: 77.01 and new cost is: 73.19 in 1 step of seeding.\n",
      "NMI score for AE is: 76.50 and new cost is: 72.76 in 2 step of seeding.\n",
      "NMI score for AE is: 79.67 and new cost is: 73.69 in 3 step of seeding.\n",
      "NMI score for AE is: 77.75 and new cost is: 73.43 in 4 step of seeding.\n",
      "NMI score for AE is: 76.01 and new cost is: 73.32 in 5 step of seeding.\n",
      "NMI score for AE is: 76.30 and new cost is: 73.46 in 6 step of seeding.\n",
      "NMI score for AE is: 76.33 and new cost is: 72.77 in 7 step of seeding.\n",
      "NMI score for AE is: 78.91 and new cost is: 73.46 in 8 step of seeding.\n",
      "NMI score for AE is: 76.76 and new cost is: 72.95 in 9 step of seeding.\n",
      "NMI score for AE is: 77.82 and new cost is: 73.48 in 10 step of seeding.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    with tf.Graph().as_default():    \n",
    "        with tf.variable_scope(\"autoencoder_architecture\"):\n",
    "            x = tf.placeholder(\"float\", [None, n_input])   \n",
    "            mode_train = tf.placeholder(tf.bool)\n",
    "            code = encoder(x, int(n_code), mode_train)\n",
    "            reconstructed = decoder(code, int(n_code), mode_train)\n",
    "            cost = loss(reconstructed, x)\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
    "            sess = tf.Session()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            # Training cycle\n",
    "            for ii in range(n_layers):\n",
    "                # Fit training with backpropagation using batch data.\n",
    "                for jj in range(n_backpro):\n",
    "                    miniData, _ = trainSet_cosine.next_batch(n_batch)\n",
    "                    _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n",
    "                                                                              mode_train: True})       \n",
    "                #------------------------- End of the Optimization ------------------------------\n",
    "\n",
    "    # Getting embedded codes and running K-Means on them.\n",
    "    ae_codes_cos = sess.run(code, feed_dict={x: data_cos, mode_train: False})        \n",
    "    idx_cos = k_means_(ae_codes_cos, n_clusters)\n",
    "    ae_nmi_cos = normalized_mutual_info_score(labels_cos, idx_cos)\n",
    "    ae_nmi_cos = ae_nmi_cos*100\n",
    "    results_cos.append(ae_nmi_cos)    \n",
    "    seeding_cos.append(i)\n",
    "    loss_cost_cos.append(new_cost)    \n",
    "    print(\"NMI score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step of seeding.\"\n",
    "          .format(ae_nmi_cos, new_cost, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average of NMI Score for >>> 10 <<< Random Factors in Autoencoder Cosine is >>> 77.31 <<<\n"
     ]
    }
   ],
   "source": [
    "print(\"The Average of NMI Score for >>> {:d} <<< Random Factors in Autoencoder Cosine is >>> {:0.2f} <<<\"\n",
    "      .format(len(seeding_cos), (np.mean(results_cos))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77.007590066411282,\n",
       " 76.504352549867065,\n",
       " 79.668435637511664,\n",
       " 77.750027835788387,\n",
       " 76.012372141698464,\n",
       " 76.298278587623102,\n",
       " 76.333008295098523,\n",
       " 78.908850902829471,\n",
       " 76.756575375770851,\n",
       " 77.821649441779954]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nliCMMcZ4sgRhjDHGkyUIY4wxnv4/UKNFOwl8uHEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b603ecf400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.ylim(30,101)\n",
    "plt.plot(seeding_cor, results_cor, label='Autoencoder Correlation Simialrity', color='m', marker='o')\n",
    "plt.plot(seeding_cos, results_cos, label='Autoencoder Cosine Simialrity', color='g', marker='s')\n",
    "plt.xlabel('Number of Seeding.')\n",
    "plt.ylabel('NMI')\n",
    "plt.grid()\n",
    "plt.title('The Average of NMI Scores')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Clustering on Cosine: ------------ 77.31\n",
      "Autoencoder Clustering on Correlation: ------- 83.37\n"
     ]
    }
   ],
   "source": [
    "print(\"Autoencoder Clustering on Cosine: ------------ {:0.2f}\".format(np.mean(results_cos)))\n",
    "print(\"Autoencoder Clustering on Correlation: ------- {:0.2f}\".format(np.mean(results_cor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refrences:\n",
    "\n",
    "SciPy (the library) Jones E, Oliphant E, Peterson P, et al. SciPy: Open Source Scientific Tools for Python, 2001-, http://www.scipy.org/ [Online; accessed 2018-02-20]. Here’s an example of a BibTeX entry:\n",
    "\n",
    "@Misc{, author = {Eric Jones and Travis Oliphant and Pearu Peterson and others}, title = {{SciPy}: Open source scientific tools for {Python}}, year = {2001--}, url = \"http://www.scipy.org/\", note = {[Online; accessed\n",
    "\n",
    "NumPy & SciPy:\n",
    "\n",
    "Stéfan van der Walt, S. Chris Colbert and Gaël Varoquaux. The NumPy Array: A Structure for Efficient Numerical Computation, Computing in Science & Engineering, 13, 22-30 (2011), DOI:10.1109/MCSE.2011.37 (publisher link)\n",
    "\n",
    "IPython:\n",
    "\n",
    "Fernando Pérez and Brian E. Granger. IPython: A System for Interactive Scientific Computing, Computing in Science & Engineering, 9, 21-29 (2007), DOI:10.1109/MCSE.2007.53 (publisher link)\n",
    "\n",
    "Matplotlib:\n",
    "\n",
    "John D. Hunter. Matplotlib: A 2D Graphics Environment, Computing in Science & Engineering, 9, 90-95 (2007), DOI:10.1109/MCSE.2007.55 (publisher link)\n",
    "\n",
    "Scikit-learn:\n",
    "\n",
    "Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay. Scikit-learn: Machine Learning in Python, Journal of Machine Learning Research, 12, 2825-2830 (2011) (publisher link)\n",
    "\n",
    "TensorFlow:\n",
    "\n",
    "@misc{tensorflow2015-whitepaper, title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems}, url={https://www.tensorflow.org/}, note={Software available from tensorflow.org},\n",
    "\n",
    "Jupyter Notebooks:\n",
    "\n",
    "@conference{Kluyver:2016aa, Author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\\'a}n Avila and Safia Abdalla and Carol Willing}, Booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas}, Editor = {F. Loizides and B. Schmidt}, Organization = {IOS Press}, Pages = {87 - 90}, Title = {Jupyter Notebooks -- a publishing format for reproducible computational workflows}, Year = {2016}}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
